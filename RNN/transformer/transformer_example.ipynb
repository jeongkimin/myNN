{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N278UAPWUSOJ",
        "outputId": "43134060-d2a5-4aab-df8d-b6a5af544076"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 10.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.95 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK50z9YuUSwu"
      },
      "source": [
        "%%capture\r\n",
        "!python -m spacy download en\r\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5NfwigFUsdJ",
        "outputId": "e610782f-c8fd-4634-ec15-e945d82dc3f9"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "if device == 'cuda':\r\n",
        "    gpu_info = !nvidia-smi\r\n",
        "    gpu_info = '\\n'.join(gpu_info)\r\n",
        "    if gpu_info.find('failed') >= 0:\r\n",
        "        print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\r\n",
        "        print('and then re-execute this cell.')\r\n",
        "    else:\r\n",
        "        print(gpu_info)\r\n",
        "print('device :',device)\r\n",
        "print('torch.version :',torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 15 18:02:30 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    27W /  70W |   1122MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "device : cuda\n",
            "torch.version : 1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZu9m1TbUSyE"
      },
      "source": [
        "import spacy\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "\r\n",
        "spacy_en = spacy.load('en') # 영어 토큰화(tokenization)\r\n",
        "spacy_de = spacy.load('de') # 독일어 토큰화(tokenization)\r\n",
        "\r\n",
        "\r\n",
        "def tokenize_de(text):\r\n",
        "    return [token.text for token in spacy_de.tokenizer(text)]\r\n",
        "\r\n",
        "# 영어(English) 문장을 토큰화 하는 함수\r\n",
        "def tokenize_en(text):\r\n",
        "    return [token.text for token in spacy_en.tokenizer(text)]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "SRC = Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)\r\n",
        "TRG = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)\r\n",
        "train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=(\".de\", \".en\"), fields=(SRC, TRG))\r\n",
        "\r\n",
        "\r\n",
        "SRC.build_vocab(train_dataset, min_freq=2)\r\n",
        "TRG.build_vocab(train_dataset, min_freq=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ecq_8_MDUS1m"
      },
      "source": [
        "mbsz = 128\r\n",
        "train_iterator, test_iterator = BucketIterator.splits((train_dataset, test_dataset), batch_size=mbsz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V37orHrvCUoW",
        "outputId": "833da45a-3036-44cb-de51-03990f089ec6"
      },
      "source": [
        "print(len(train_dataset))\r\n",
        "print(len(valid_dataset))\r\n",
        "print(len(test_dataset))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29000\n",
            "1014\n",
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKu61Fr6aDEg"
      },
      "source": [
        "class PositionalEmbedding(object):\r\n",
        "    def __init__(self, n_seq, d_hid):\r\n",
        "        self.n_seq, self.d_hid = n_seq, d_hid\r\n",
        "        self.table = torch.zeros(1, n_seq, d_hid).to(device)\r\n",
        "        self.fn = [np.sin, np.cos]\r\n",
        "        for t in range(n_seq):\r\n",
        "            for i in range(d_hid):\r\n",
        "                self.table[0][t][i] = self.fn[i % 2](t/np.power(10000, (i // 2)/d_hid))\r\n",
        "\r\n",
        "    def get(self, batch_size, n_seq):\r\n",
        "        if n_seq > self.n_seq:\r\n",
        "            new_pos = torch.zeros(1, n_seq - self.n_seq, self.d_hid).to(device)\r\n",
        "            for t in range(n_seq - self.n_seq):\r\n",
        "                for i in range(self.d_hid):\r\n",
        "                    new_pos[0][t][i] = self.fn[i % 2]((t + self.n_seq)/np.power(10000, (i // 2)/self.d_hid))\r\n",
        "            \r\n",
        "            self.table = torch.cat([self.table, new_pos], dim=1)\r\n",
        "            self.n_seq = n_seq\r\n",
        "        return self.table[:, :n_seq, :].expand(batch_size, -1, -1)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OohyplvHnB_i"
      },
      "source": [
        "class EncoderBlock(nn.Module):\r\n",
        "    def __init__(self, d_hidn, n_head, dropout_ratio):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        assert d_hidn % n_head == 0\r\n",
        "\r\n",
        "        self.d_hidn = d_hidn\r\n",
        "        self.d_head = d_hidn // n_head\r\n",
        "        self.n_head = n_head\r\n",
        "        self.projs = nn.Parameter(nn.init.xavier_uniform_(torch.empty(d_hidn, d_hidn * 3)))\r\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\r\n",
        "        self.layer_norm1 = nn.LayerNorm(d_hidn)\r\n",
        "        self.feedforward = nn.Sequential(nn.Linear(d_hidn, d_hidn),\r\n",
        "                                         nn.ReLU(),\r\n",
        "                                         nn.Linear(d_hidn, d_hidn))\r\n",
        "        self.layer_norm2 = nn.LayerNorm(d_hidn)\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x, mask): # mask.shape = [mbsz, 1, n_seq, n_seq]\r\n",
        "        \r\n",
        "        mbsz, n_seq = x.size(0), x.size(1)\r\n",
        "        key, que, val = (x @ self.projs).chunk(3, 2)\r\n",
        "        key = key.view(mbsz, n_seq, self.n_head, -1).permute(0, 2, 1, 3).contiguous() #[mbsz, n_head, n_seq, d_head]\r\n",
        "        que = que.view(mbsz, n_seq, self.n_head, -1).permute(0, 2, 3, 1).contiguous() #[mbsz, n_head, d_head, n_seq]\r\n",
        "        val = val.view(mbsz, n_seq, self.n_head, -1).permute(0, 2, 1, 3).contiguous() #[mbsz, n_head, n_seq, d_head]\r\n",
        "\r\n",
        "        x1 = torch.matmul(key, que / np.sqrt(self.d_head)) #[mbsz, n_head, n_seq, n_seq']\r\n",
        "        x1 = x1.masked_fill(mask, -np.inf)\r\n",
        "        x1 = torch.softmax(x1, dim=3)\r\n",
        "        x1 = self.dropout(x1)\r\n",
        "        x1 = torch.matmul(x1, val) #[mbsz, n_head, n_seq, n_seq']\r\n",
        "        x1 = x1.permute(0, 2, 3, 1).contiguous().flatten(2, 3)\r\n",
        "\r\n",
        "        x = (x + x1).view(-1, self.d_hidn) #[mbsz * n_seq, d_hidn]\r\n",
        "        x = self.layer_norm1(x)\r\n",
        "        x = x + self.feedforward(x)\r\n",
        "        x = self.layer_norm2(x)\r\n",
        "        x = x.view(mbsz, n_seq, -1)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, d_input, d_hidn, num_layers, n_head, dropout_ratio):\r\n",
        "        super().__init__()\r\n",
        "        self.embedding = nn.Embedding(d_input, d_hidn)\r\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\r\n",
        "        self.positional_embedding = PositionalEmbedding(n_seq=2, d_hid=d_hidn)\r\n",
        "        self.layers = nn.ModuleList([EncoderBlock(d_hidn, n_head, dropout_ratio) for _ in range(num_layers)])\r\n",
        "        \r\n",
        "\r\n",
        "    def forward(self, x, mask):\r\n",
        "\r\n",
        "        x = self.embedding(x)\r\n",
        "        x += self.positional_embedding.get(x.size(0), x.size(1))\r\n",
        "        x = self.dropout(x)\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "            x = layer(x, mask)\r\n",
        "\r\n",
        "        return x\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37G_SqU0nEMO"
      },
      "source": [
        "class DecoderBlock(nn.Module):\r\n",
        "    def __init__(self, d_hidn, n_head, dropout_ratio, enc_d_hidn, enc_n_head):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        assert d_hidn % n_head == 0\r\n",
        "\r\n",
        "        self.d_hidn = d_hidn\r\n",
        "        self.d_head = d_hidn // n_head\r\n",
        "        self.n_head = n_head\r\n",
        "\r\n",
        "        self.enc_d_hidn = enc_d_hidn\r\n",
        "        self.enc_n_head = enc_n_head\r\n",
        "\r\n",
        "        self.projs = nn.Parameter(nn.init.xavier_uniform_(torch.empty(d_hidn, d_hidn * 3)))\r\n",
        "        self.to_query = nn.Linear(d_hidn, enc_d_hidn)\r\n",
        "                \r\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\r\n",
        "        self.layer_norm1 = nn.LayerNorm(d_hidn)\r\n",
        "        \r\n",
        "        self.enc_projs = nn.Parameter(nn.init.xavier_uniform_(torch.empty(enc_d_hidn, enc_d_hidn * 2)))\r\n",
        "\r\n",
        "        self.enc_to_dec = nn.Linear(enc_d_hidn, d_hidn)\r\n",
        "    \r\n",
        "\r\n",
        "        self.feedforward = nn.Sequential(nn.Linear(d_hidn, d_hidn),\r\n",
        "                                         nn.ReLU(),\r\n",
        "                                         nn.Linear(d_hidn, d_hidn))\r\n",
        "        self.layer_norm2 = nn.LayerNorm(d_hidn)\r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x, enc_out, trg_mask, enc_mask): # mask.shape = [mbsz, 1, n_seq, n_seq]\r\n",
        "        \r\n",
        "        mbsz, n_seq = x.size(0), x.size(1)\r\n",
        "        enc_n_seq = enc_out.size(1)\r\n",
        "        key, que, val = (x @ self.projs).chunk(3, 2)\r\n",
        "        key = key.view(mbsz, n_seq, self.n_head, -1).permute(0, 2, 1, 3).contiguous() #[mbsz, n_head, n_seq, d_head]\r\n",
        "        que = que.view(mbsz, n_seq, self.n_head, -1).permute(0, 2, 3, 1).contiguous() #[mbsz, n_head, d_head, n_seq]\r\n",
        "        val = val.view(mbsz, n_seq, self.n_head, -1).permute(0, 2, 1, 3).contiguous() #[mbsz, n_head, n_seq, d_head]\r\n",
        "\r\n",
        "        x1 = torch.matmul(key, que) / np.sqrt(self.d_head) #[mbsz, n_head, n_seq, n_seq']\r\n",
        "        x1 = x1.masked_fill(trg_mask, -np.inf)\r\n",
        "        x1 = torch.softmax(x1, dim=3)\r\n",
        "        x1 = self.dropout(x1)\r\n",
        "        x1 = torch.matmul(x1, val) #[mbsz, n_head, n_seq, n_seq']\r\n",
        "        x1 = x1.permute(0, 2, 3, 1).contiguous().flatten(2, 3) #[mbsz, n_seq, n_head x n_seq']\r\n",
        "\r\n",
        "\r\n",
        "        x = (x + x1).view(-1, self.d_hidn) #[mbsz * n_seq, d_hidn]\r\n",
        "        x = self.layer_norm1(x)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        dec_que = self.to_query(x) #[mbsz, dec_n_seq, enc_d_hidn]\r\n",
        "\r\n",
        "        dec_que = dec_que.view(mbsz, n_seq, self.enc_n_head, -1).permute(0, 2, 1, 3).contiguous() #[mbsz, enc_n_head, dec_n_seq, enc_d_head]\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "        enc_key, enc_val = (enc_out @ self.enc_projs).chunk(2, 2)\r\n",
        " \r\n",
        "        enc_key = enc_key.view(mbsz, enc_n_seq, self.enc_n_head, -1).permute(0, 2, 3, 1).contiguous() #[mbsz, enc_n_head, enc_d_head, enc_n_seq]\r\n",
        "        enc_val = enc_val.view(mbsz, enc_n_seq, self.enc_n_head, -1).permute(0, 2, 1, 3).contiguous() #[mbsz, enc_n_head, enc_n_seq, enc_d_head]\r\n",
        "\r\n",
        "\r\n",
        "        x2 = torch.matmul(dec_que, enc_key) / np.sqrt(self.enc_d_hidn / self.enc_n_head) #[mbsz, enc_n_head, dec_n_seq, enc_n_seq]\r\n",
        "\r\n",
        "\r\n",
        "        x2 = x2.masked_fill(enc_mask, -np.inf)\r\n",
        "\r\n",
        "        x2 = torch.softmax(x2, dim=3)\r\n",
        "\r\n",
        "\r\n",
        "        x2 = torch.matmul(x2, enc_val) #[mbsz, enc_n_head, dec_n_seq, enc_d_head]\r\n",
        "        x2 = x2.permute(0, 2, 3, 1).contiguous().flatten(2, 3) #[mbsz, dec_n_seq, enc_n_head x enc_d_head']\r\n",
        "\r\n",
        "        x2 = x2.view(-1, self.enc_d_hidn)\r\n",
        "        x2 = self.enc_to_dec(x2)\r\n",
        "        \r\n",
        "        x = x + x2\r\n",
        "\r\n",
        "        x = x + self.feedforward(x)\r\n",
        "        x = self.layer_norm2(x)\r\n",
        "        x = x.view(mbsz, n_seq, -1)\r\n",
        "        return x\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, d_input, d_hidn, d_out, num_layers, n_head, dropout_ratio, enc_d_hidn, enc_n_head):\r\n",
        "        super().__init__()\r\n",
        "        self.embedding = nn.Embedding(d_input, d_hidn)\r\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\r\n",
        "        self.positional_embedding = PositionalEmbedding(n_seq=2, d_hid=d_hidn)\r\n",
        "        self.layers = nn.ModuleList([DecoderBlock(d_hidn, n_head, dropout_ratio, enc_d_hidn, enc_n_head) for _ in range(num_layers)])\r\n",
        "        self.decoder_out = nn.Linear(d_hidn, d_out)\r\n",
        "\r\n",
        "    def forward(self, x, enc_out, trg_mask, enc_mask):\r\n",
        "\r\n",
        "        mbsz, n_seq = x.size(0), x.size(1)\r\n",
        "        x = self.embedding(x)\r\n",
        "        x += self.positional_embedding.get(x.size(0), x.size(1))\r\n",
        "        x = self.dropout(x)\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "            x = layer(x, enc_out, trg_mask, enc_mask)\r\n",
        "        \r\n",
        "        x = x.view(mbsz * n_seq, -1)\r\n",
        "        x = self.decoder_out(x)\r\n",
        "        x = x.view(mbsz, n_seq, -1)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YzAngyCnHjx"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "HIDDEN_DIM = 512\r\n",
        "ENC_LAYERS = 6\r\n",
        "DEC_LAYERS = 6\r\n",
        "ENC_HEADS = 8\r\n",
        "DEC_HEADS = 8\r\n",
        "\r\n",
        "ENC_DROPOUT = 0.1\r\n",
        "DEC_DROPOUT = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srC4f6NanIUj"
      },
      "source": [
        "enc = Encoder(d_input=INPUT_DIM,  d_hidn=HIDDEN_DIM, num_layers=ENC_LAYERS, n_head=ENC_HEADS, dropout_ratio=ENC_DROPOUT).to(device)\r\n",
        "dec = Decoder(d_input=OUTPUT_DIM, d_hidn=HIDDEN_DIM, d_out=OUTPUT_DIM, num_layers=DEC_LAYERS, n_head=DEC_HEADS, dropout_ratio=DEC_DROPOUT, enc_d_hidn=HIDDEN_DIM, enc_n_head=ENC_HEADS).to(device)\r\n",
        "\r\n",
        "\r\n",
        "enc_solver = optim.Adam(enc.parameters(), lr=0.0005)\r\n",
        "dec_solver = optim.Adam(dec.parameters(), lr=0.0005)\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG.vocab.stoi['<pad>'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGPuub6XVh_3",
        "outputId": "0e02422b-6a96-4f8e-ffc6-7a9ace19eeb7"
      },
      "source": [
        "num_epochs = 20\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    train_loss = 0.0\r\n",
        "    for i, batch in enumerate(train_iterator):\r\n",
        "        src = batch.src.to(device)\r\n",
        "        trg_ = batch.trg.to(device)\r\n",
        "        \r\n",
        "        trg = trg_[:, :-1]\r\n",
        "\r\n",
        "        target = trg_[:, 1:]\r\n",
        "\r\n",
        "        src_data_pad = src == SRC.vocab.stoi['<pad>']\r\n",
        "        trg_data_pad = trg == TRG.vocab.stoi['<pad>']\r\n",
        "\r\n",
        "        enc_mask = src_data_pad.unsqueeze(1).expand(-1, src.size(1), -1).unsqueeze(1).to(device)\r\n",
        "\r\n",
        "        dec_mask = torch.tril(torch.ones(trg.size(1), trg.size(1)), diagonal=-1).transpose(0, 1)\r\n",
        "        dec_mask = dec_mask.unsqueeze(0).expand(trg.size(0), -1, -1).to(device)\r\n",
        "        dec_mask = dec_mask.logical_or(trg_data_pad.unsqueeze(1).expand(-1, trg.size(1), -1).to(device)).unsqueeze(1)\r\n",
        "\r\n",
        "        dec_src_mask = src_data_pad.unsqueeze(1).expand(-1, trg.size(1), -1).unsqueeze(1).to(device)\r\n",
        "\r\n",
        "        enc_solver.zero_grad()\r\n",
        "        dec_solver.zero_grad()\r\n",
        "        encoded = enc(src, enc_mask)\r\n",
        "        decoded = dec(trg, encoded, dec_mask, dec_src_mask)\r\n",
        "        \r\n",
        "        loss = criterion(decoded.flatten(0, 1), target.flatten(0, 1))\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        enc_solver.step()\r\n",
        "        dec_solver.step()\r\n",
        "        train_loss += loss.item()\r\n",
        "    print(train_loss / len(train_iterator))\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09564689103727299\n",
            "0.09580299742355745\n",
            "0.09529580961962103\n",
            "0.09508279907939718\n",
            "0.09416684388523584\n",
            "0.08791264235185632\n",
            "0.08960615868150926\n",
            "0.08981538905314937\n",
            "0.09041737760150485\n",
            "0.09108656492199141\n",
            "0.09055035999496078\n",
            "0.08808559026684004\n",
            "0.08582361039217348\n",
            "0.08288997667739045\n",
            "0.08471085244404061\n",
            "0.08546623507194583\n",
            "0.08675657055188905\n",
            "0.0813773715712688\n",
            "0.0821534363872942\n",
            "0.08224148723391184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C-XawFnsPE1",
        "outputId": "03f725f0-44dd-4695-92af-b52a75140fba"
      },
      "source": [
        "example_idx = 2\r\n",
        "\r\n",
        "src = vars(test_dataset.examples[example_idx])['src']\r\n",
        "trg = vars(test_dataset.examples[example_idx])['trg']\r\n",
        "\r\n",
        "print(\" \".join(src))\r\n",
        "print(\" \".join(trg))\r\n",
        "\r\n",
        "src = [SRC.vocab.stoi['<sos>']] + [SRC.vocab.stoi[token.lower()] for token in src] + [SRC.vocab.stoi['<eos>']]\r\n",
        "src = torch.LongTensor(src).unsqueeze(0).to(device)\r\n",
        "enc_mask = torch.zeros(1, src.size(1), src.size(1)).bool().to(device)\r\n",
        "\r\n",
        "enc.eval()\r\n",
        "\r\n",
        "encoded = enc(src, enc_mask)\r\n",
        "max_len = 50\r\n",
        "\r\n",
        "target = torch.zeros(1, max_len).long().to(device)\r\n",
        "target[0][0] = TRG.vocab.stoi[\"<sos>\"]\r\n",
        "\r\n",
        "dec_mask = torch.tril(torch.ones(max_len, max_len), diagonal=-1).transpose(0, 1)\r\n",
        "dec_mask = dec_mask.unsqueeze(0).bool().to(device)\r\n",
        "dec_src_mask = torch.zeros(1, max_len, src.size(1)).bool().to(device)\r\n",
        "\r\n",
        "enc.train()\r\n",
        "\r\n",
        "dec.eval()\r\n",
        "\r\n",
        "\r\n",
        "seq = []\r\n",
        "for i in range(max_len):\r\n",
        "    decoded = dec(target, encoded, dec_mask, dec_src_mask)\r\n",
        "    argmax = decoded[0][i].argmax().item()\r\n",
        "    \r\n",
        "    if i != max_len - 1:\r\n",
        "        target[0][i + 1] = argmax\r\n",
        "\r\n",
        "    if argmax == TRG.vocab.stoi['<eos>']:\r\n",
        "        break\r\n",
        "\r\n",
        "    seq.append(argmax)\r\n",
        "\r\n",
        "dec.train()\r\n",
        "\r\n",
        "seq_trans = [TRG.vocab.itos[idx] for idx in seq]\r\n",
        "print(\" \".join(seq_trans))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ein mädchen in einem karateanzug bricht ein brett mit einem tritt .\n",
            "a girl in karate uniform breaking a stick with a front kick .\n",
            "a girl in a karate uniform uses a snowboard to toe .\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}